{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb17e923",
   "metadata": {},
   "source": [
    "# Continuous Transformer\n",
    "What if we take the Transformer to its continuous limit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63ee65a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings\n",
    "DIM = 1024\n",
    "DEPTH = 6\n",
    "VOCAB_SIZE = 256\n",
    "USE_CHECKPOINT = True  # Gradient checkpointing\n",
    "\n",
    "# Training settings\n",
    "BASE_LR = 3e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "BATCH_SIZE = 1\n",
    "SEQ_LENGTH = 8192 # Character level\n",
    "GRAD_CLIP = 1.0\n",
    "\n",
    "# Checkpoint settings\n",
    "CHECKPOINT_EVERY = 1000\n",
    "PRINT_EVERY = 100\n",
    "\n",
    "# Generation settings\n",
    "GEN_LENGTH = 200\n",
    "GEN_TEMPERATURE = 0.8\n",
    "GEN_TOP_P = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c4c4996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import urllib.request\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from tqdm import tqdm\n",
    "import mmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84ef3635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tinystories_dataset(cache_dir='~/.cache/continuous_transformer'):\n",
    "    \"\"\"\n",
    "    Download TinyStories dataset if needed.\n",
    "    Returns path to the text file - no preprocessing needed!\n",
    "    \"\"\"\n",
    "    cache_dir = os.path.expanduser(cache_dir)\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    train_path = os.path.join(cache_dir, 'tinystories_train.txt')\n",
    "    \n",
    "    if not os.path.exists(train_path):\n",
    "        print(\"Downloading TinyStories training data...\")\n",
    "        url = \"https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt\"\n",
    "        urllib.request.urlretrieve(url, train_path)\n",
    "        print(f\"Downloaded to {train_path}\")\n",
    "    \n",
    "    file_size = os.path.getsize(train_path)\n",
    "    print(f\"Using dataset: {train_path} ({file_size/1e6:.1f}MB)\")\n",
    "    \n",
    "    return train_path\n",
    "\n",
    "\n",
    "class TinyStoriesDataset(Dataset):\n",
    "    def __init__(self, data_path: str, seq_length: int, stride: int = 512):\n",
    "        self.data_path = data_path\n",
    "        self.seq_length = seq_length\n",
    "        self.stride = stride\n",
    "        \n",
    "        # Get file size without loading into memory\n",
    "        self.file_size = os.path.getsize(data_path)\n",
    "        \n",
    "        # Calculate dataset length with stride to reduce overlapping sequences\n",
    "        self.length = (self.file_size - seq_length - 1) // stride\n",
    "        \n",
    "        # Lazy initialization - file opened on first access per worker\n",
    "        self._file = None\n",
    "        self._mmap = None\n",
    "        \n",
    "    def _ensure_mmap(self):\n",
    "        \"\"\"Lazy open file handle - called once per worker process\"\"\"\n",
    "        if self._mmap is None:\n",
    "            self._file = open(self.data_path, 'rb')  # Binary mode for mmap\n",
    "            self._mmap = mmap.mmap(self._file.fileno(), 0, access=mmap.ACCESS_READ)\n",
    "        return self._mmap\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Stream a single sequence from disk\n",
    "        \n",
    "        Args:\n",
    "            idx: Sequence index\n",
    "            \n",
    "        Returns:\n",
    "            x: Input tokens [L]\n",
    "            y: Target tokens [L]\n",
    "        \"\"\"\n",
    "        # Get mmap (opens file if this is first call in this worker)\n",
    "        mm = self._ensure_mmap()\n",
    "        \n",
    "        # Calculate file position with stride\n",
    "        pos = idx * self.stride\n",
    "        \n",
    "        # Read only what we need from disk\n",
    "        mm.seek(pos)\n",
    "        chunk = mm.read(self.seq_length + 1)\n",
    "        \n",
    "        # Decode and filter to ASCII\n",
    "        try:\n",
    "            text = chunk.decode('utf-8', errors='ignore')\n",
    "        except:\n",
    "            text = chunk.decode('latin-1', errors='ignore')\n",
    "        \n",
    "        # Keep only ASCII characters\n",
    "        text = ''.join(c for c in text if ord(c) < 128)\n",
    "        \n",
    "        # Pad if needed (near end of file)\n",
    "        if len(text) < self.seq_length + 1:\n",
    "            text = text + ' ' * (self.seq_length + 1 - len(text))\n",
    "        \n",
    "        # Convert to tokens\n",
    "        x = torch.tensor([ord(c) for c in text[:self.seq_length]], dtype=torch.long)\n",
    "        y = torch.tensor([ord(c) for c in text[1:self.seq_length + 1]], dtype=torch.long)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def __del__(self):\n",
    "        \"\"\"Clean up file handles\"\"\"\n",
    "        if self._mmap is not None:\n",
    "            self._mmap.close()\n",
    "        if self._file is not None:\n",
    "            self._file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da6d6745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset: /home/midori/.cache/continuous_transformer/tinystories_train.txt (2227.8MB)\n"
     ]
    }
   ],
   "source": [
    "# Setup dataset\n",
    "data_path = prepare_tinystories_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1acf564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hippo_freqs(dim: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute HiPPO-LegS frequency initialization for optimal history reconstruction.\n",
    "    \n",
    "    Args:\n",
    "        dim: Hidden dimension size\n",
    "        \n",
    "    Returns:\n",
    "        Frequency tensor of shape [dim]\n",
    "    \"\"\"\n",
    "    n = torch.arange(dim)\n",
    "    freqs = (2 * n + 1) ** 0.5\n",
    "    freqs = freqs / freqs.max()\n",
    "    return torch.exp(-freqs * np.log(10_000))\n",
    "\n",
    "def exact_projection(z: torch.Tensor, target_mean: float = 0.0, target_std: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Exact closed-form projection onto stable manifold S1(σ₁, σ₂).\n",
    "    Geodesic projection solving the normalization constraint analytically.\n",
    "    \n",
    "    Args:\n",
    "        z: Input tensor of shape [*, D]\n",
    "        target_mean: Target mean σ₁\n",
    "        target_std: Target standard deviation σ₂\n",
    "        \n",
    "    Returns:\n",
    "        Projected tensor of shape [*, D]\n",
    "    \"\"\"\n",
    "    mean = z.mean(dim=-1, keepdim=True)  # α(x)\n",
    "    var = ((z - mean) ** 2).mean(dim=-1, keepdim=True)  # β(x)\n",
    "    z_projected = (z - mean) / (torch.sqrt(var) + 1e-6) * target_std + target_mean\n",
    "    return z_projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2cf3c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralField(nn.Module):\n",
    "    \"\"\"\n",
    "    Recursive operator combining spatial, interaction, and temporal transformations.\n",
    "    Implements: z_{t+1} = Π(z_t + dt · F[z_t])\n",
    "    where F is a composition of rational filtering, gating, and temporal convolution.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Rational field parameters: R(z) = F·z / (|G·z| + ε)\n",
    "        self.F = nn.Parameter(torch.complex(torch.randn(dim), torch.randn(dim)) * 0.02)\n",
    "        self.G = nn.Parameter(torch.complex(torch.randn(dim), torch.randn(dim)) * 0.02)\n",
    "        \n",
    "        # Gating parameters: σ(z) = tanh(α·|z| + β·φ)\n",
    "        self.alpha = nn.Parameter(torch.ones(dim) * 0.5)  # Content coefficient\n",
    "        self.beta = nn.Parameter(torch.zeros(dim))  # Position coefficient\n",
    "        \n",
    "        # Temporal kernel parameters: K(t) = exp((-γ + iω)t)\n",
    "        self.log_gamma = nn.Parameter(torch.randn(dim) * 0.5 - 2.0)  # Decay rate\n",
    "        self.omega = nn.Parameter(hippo_freqs(dim))  # Base frequencies\n",
    "        self.omega_mod = nn.Parameter(torch.randn(dim) * 0.02)  # Frequency modulation\n",
    "        \n",
    "        self.dt = nn.Parameter(torch.tensor(0.1))  # Integration timestep\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            z: Input tensor [B, L, D]\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor [B, L, D]\n",
    "        \"\"\"\n",
    "        B, L, D = z.shape\n",
    "        \n",
    "        # Spatial transformation in frequency domain\n",
    "        Z_x = torch.fft.fft(z, dim=-1)  # [B, L, D]\n",
    "        denom = torch.abs(Z_x * self.G) + 0.1\n",
    "        Z_filtered = Z_x * self.F / denom\n",
    "        z_spatial = torch.fft.ifft(Z_filtered, dim=-1)  # [B, L, D]\n",
    "        \n",
    "        # Position-aware gating\n",
    "        t = torch.arange(L, device=z.device)  # [L]\n",
    "        phi = (t.view(L, 1) * self.omega.view(1, D) + math.pi) % (2*math.pi) - math.pi  # [L, D]\n",
    "        intensity = z_spatial.abs()  # [B, L, D]\n",
    "        gate = torch.tanh(intensity * self.alpha + phi * self.beta)  # [B, L, D]\n",
    "        z_gated = z_spatial * gate  # [B, L, D]\n",
    "        \n",
    "        # Temporal convolution via spectral multiplication\n",
    "        Z_t = torch.fft.fft(z_gated, dim=1)  # [B, L, D]\n",
    "        omega_total = self.omega + torch.sin(self.omega_mod) * 0.1  # [D]\n",
    "        lambda_complex = torch.complex(-torch.exp(self.log_gamma), omega_total)  # [D]\n",
    "        kernel = torch.exp(lambda_complex * t.unsqueeze(1))  # [L, D]\n",
    "        K_freq = torch.fft.fft(kernel, dim=0, n=L)  # [L, D]\n",
    "        dz = torch.fft.ifft(Z_t * K_freq.unsqueeze(0), dim=1)  # [B, L, D]\n",
    "        \n",
    "        # Euler integration with manifold projection\n",
    "        z_next = z + dz * self.dt  # [B, L, D]\n",
    "        z_next = exact_projection(z_next)  # [B, L, D]\n",
    "        \n",
    "        return z_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96c1d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Continuous-time transformer with recursive operator application.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int = 1024, depth: int = 6, vocab: int = 256, use_checkpoint: bool = True):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab, dim)\n",
    "        self.to_complex = nn.Linear(dim, dim * 2)\n",
    "        self.operator = SpectralField(dim)\n",
    "        self.depth_emb = nn.Parameter(torch.randn(depth, dim * 2) * 0.02)\n",
    "        self.out_re = nn.Linear(dim, vocab)\n",
    "        self.out_im = nn.Linear(dim, vocab)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input token indices [B, L]\n",
    "            \n",
    "        Returns:\n",
    "            logits: Output logits [B, L, V]\n",
    "            z: Final complex state [B, L, D]\n",
    "        \"\"\"\n",
    "        # Token embedding to complex space\n",
    "        emb = self.embed(x)  # [B, L, D]\n",
    "        z_raw = self.to_complex(emb)  # [B, L, 2D]\n",
    "        z = torch.complex(z_raw[..., :self.dim], z_raw[..., self.dim:])  # [B, L, D]\n",
    "        \n",
    "        # Recursive operator application\n",
    "        for i in range(self.depth):\n",
    "            # Depth-dependent bias injection\n",
    "            d_bias = self.depth_emb[i]  # [2D]\n",
    "            z = z + torch.complex(d_bias[:self.dim], d_bias[self.dim:])  # [B, L, D]\n",
    "            \n",
    "            # Apply operator with optional gradient checkpointing\n",
    "            if self.training and self.use_checkpoint:\n",
    "                z = checkpoint(self.operator, z, use_reentrant=False)\n",
    "            else:\n",
    "                z = self.operator(z)\n",
    "        \n",
    "        # Project to vocabulary logits\n",
    "        logits = self.out_re(z.real) + self.out_im(z.imag)  # [B, L, V]\n",
    "        return logits, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f340244",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model: nn.Module, prompt: str, device: str = 'cuda') -> None:\n",
    "    \"\"\"\n",
    "    Autoregressive text generation.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained ContinuousTransformer instance\n",
    "        prompt: Initial text prompt\n",
    "        device: Computation device\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    tokens = [ord(c) for c in prompt]\n",
    "    print(f\"\\nGeneration: {prompt}\", end=\"\", flush=True)\n",
    "    \n",
    "    for _ in range(GEN_LENGTH):\n",
    "        ctx = torch.tensor([tokens[-1024:]], device=device)  # [1, L']\n",
    "        logits, _ = model(ctx)  # [1, L', V]\n",
    "        \n",
    "        # Nucleus (top-p) sampling\n",
    "        probs = torch.softmax(logits[0, -1] / GEN_TEMPERATURE, dim=-1)  # [V]\n",
    "        sorted_probs, idx = torch.sort(probs, descending=True)\n",
    "        cumsum = torch.cumsum(sorted_probs, dim=0)\n",
    "        cutoff = (cumsum > GEN_TOP_P).float()\n",
    "        cutoff[1:] = cutoff[:-1].clone()\n",
    "        cutoff[0] = 0\n",
    "        probs[idx[cutoff.bool()]] = 0\n",
    "        probs = probs / probs.sum()  # Renormalize\n",
    "        \n",
    "        next_token = torch.multinomial(probs, 1).item()\n",
    "        tokens.append(next_token)\n",
    "        print(chr(next_token), end=\"\", flush=True)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f55d0a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d93ae748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "Parameters: 2.91M\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TinyStoriesDataset(data_path, seq_length=SEQ_LENGTH)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Initialize model\n",
    "model = ContinuousTransformer(dim=DIM, depth=DEPTH, vocab=VOCAB_SIZE, use_checkpoint=USE_CHECKPOINT).to(device)\n",
    "params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Parameters: {params/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b1d43e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: continuous_transformer_step134641.pt\n",
      "Resuming from step 134641\n",
      "Step 134700 (Epoch 0, Batch 134700/4351064) | Loss: 0.9487 | Acc: 71.2% | LR: 1.15e-04 | Stability: 0.02\n",
      "Step 134800 (Epoch 0, Batch 134800/4351064) | Loss: 0.8964 | Acc: 72.4% | LR: 1.14e-04 | Stability: 0.01\n",
      "Step 134900 (Epoch 0, Batch 134900/4351064) | Loss: 0.8981 | Acc: 72.0% | LR: 1.15e-04 | Stability: 0.03\n",
      "Step 135000 (Epoch 0, Batch 135000/4351064) | Loss: 0.9494 | Acc: 70.4% | LR: 1.17e-04 | Stability: 0.05\n",
      "\n",
      "Generation: Once upon a time to get him. They were happy and explain and took a little bird and protend.\n",
      "<|endoftext|>\n",
      "Once upon a time, there was a little boy named Ben. Ben sail in his friends and pretty. The cat on it a littl\n",
      "\n",
      "  Checkpoint saved: step 135000\n",
      "\n",
      "Step 135100 (Epoch 0, Batch 135100/4351064) | Loss: 0.9194 | Acc: 71.4% | LR: 1.17e-04 | Stability: 0.05\n",
      "Step 135200 (Epoch 0, Batch 135200/4351064) | Loss: 0.9166 | Acc: 72.0% | LR: 1.17e-04 | Stability: 0.05\n",
      "\n",
      "Training interrupted by user\n",
      "Final step: 135211\n",
      "Final checkpoint saved: continuous_transformer_step135211.pt\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint restoration\n",
    "import glob\n",
    "checkpoints = glob.glob(\"continuous_transformer_step*.pt\")\n",
    "if checkpoints:\n",
    "    steps = [int(cp.split(\"step\")[1].split(\".\")[0]) for cp in checkpoints]\n",
    "    latest_step = max(steps)\n",
    "    latest_checkpoint = f\"continuous_transformer_step{latest_step:05d}.pt\"\n",
    "    print(f\"Loading checkpoint: {latest_checkpoint}\")\n",
    "    model.load_state_dict(torch.load(latest_checkpoint, map_location=device))\n",
    "    start_step = latest_step\n",
    "    print(f\"Resuming from step {start_step}\")\n",
    "else:\n",
    "    start_step = 0\n",
    "    print(\"No checkpoint found.\")\n",
    "\n",
    "# Optimizer with homeostatic learning rate control\n",
    "opt = optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "ema_loss = None\n",
    "\n",
    "step = start_step\n",
    "try:\n",
    "    while True:\n",
    "        for batch_idx, (x, y) in enumerate(dataloader):\n",
    "            x = x.to(device)  # [B, L]\n",
    "            y = y.to(device)  # [B, L]\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, _ = model(x)  # [B, L, V]\n",
    "            loss = nn.CrossEntropyLoss()(logits.view(-1, VOCAB_SIZE), y.view(-1))\n",
    "            \n",
    "            # Backward pass\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Homeostatic learning rate adjustment\n",
    "            with torch.no_grad():\n",
    "                # Compute gradient statistics\n",
    "                grad_norm = sum(p.grad.norm()**2 for p in model.parameters() if p.grad is not None)**0.5\n",
    "                \n",
    "                # Update exponential moving average of loss\n",
    "                if ema_loss is None: \n",
    "                    ema_loss = loss.item()\n",
    "                ema_loss = 0.95 * ema_loss + 0.05 * loss.item()\n",
    "                \n",
    "                # Adaptive learning rate based on gradient stability\n",
    "                stability = 1.0 / (grad_norm.item() + 1e-6)\n",
    "                reactive_lr = BASE_LR / (1.0 + np.exp(-(stability - 0.5)))\n",
    "                \n",
    "                # Update optimizer learning rate\n",
    "                for param_group in opt.param_groups:\n",
    "                    param_group['lr'] = reactive_lr\n",
    "            \n",
    "            # Gradient clipping and optimization step\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "            opt.step()\n",
    "            \n",
    "            # Logging\n",
    "            if step % PRINT_EVERY == 0:\n",
    "                acc = (logits.argmax(-1) == y).float().mean()\n",
    "                epoch = step // len(dataloader)\n",
    "                batch_in_epoch = step % len(dataloader)\n",
    "                print(f\"Step {step:05d} (Epoch {epoch}, Batch {batch_in_epoch}/{len(dataloader)}) | Loss: {loss.item():.4f} | Acc: {acc.item()*100:.1f}% | LR: {reactive_lr:.2e} | Stability: {stability:.2f}\")\n",
    "            \n",
    "            # Checkpointing and generation\n",
    "            if step % CHECKPOINT_EVERY == 0 and step > start_step:\n",
    "                generate(model, \"Once upon a time\", device=device)\n",
    "                torch.save(model.state_dict(), f\"continuous_transformer_step{step:05d}.pt\")\n",
    "                print(f\"  Checkpoint saved: step {step}\\n\")\n",
    "            \n",
    "            step += 1\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted by user\")\n",
    "    print(f\"Final step: {step}\")\n",
    "    \n",
    "    # Save final checkpoint\n",
    "    final_checkpoint = f\"continuous_transformer_step{step:05d}.pt\"\n",
    "    torch.save(model.state_dict(), final_checkpoint)\n",
    "    print(f\"Final checkpoint saved: {final_checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59643c0",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- **A Mathematical Explanation of Transformers for Large Language Models and GPTs** - Tai et al. (2024). [arXiv:2510.03989](https://arxiv.org/abs/2510.03989)\n",
    "- **HiPPO: Recurrent Memory with Optimal Polynomial Projections** - Gu et al. (2020). NeurIPS. [arXiv:2008.07669](https://arxiv.org/abs/2008.07669)\n",
    "- **Efficiently Modeling Long Sequences with Structured State Spaces (S4)** - Gu et al. (2022). ICLR. [arXiv:2111.00396](https://arxiv.org/abs/2111.00396)\n",
    "- **Neural Ordinary Differential Equations** - Chen et al. (2018). NeurIPS. [arXiv:1806.07366](https://arxiv.org/abs/1806.07366)\n",
    "- **Deep Complex Networks** - Trabelsi et al. (2017). ICLR. [arXiv:1705.09792](https://arxiv.org/abs/1705.09792)\n",
    "- **FNet: Mixing Tokens with Fourier Transforms** - Lee-Thorp et al. (2021). NAACL. [arXiv:2105.03824](https://arxiv.org/abs/2105.03824)\n",
    "- **Language Modeling with Gated Convolutional Networks** - Dauphin et al. (2017). ICML. [arXiv:1612.08083](https://arxiv.org/abs/1612.08083)\n",
    "- **Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting** - Zhou et al. (2021). AAAI. [arXiv:2012.07436](https://arxiv.org/abs/2012.07436)\n",
    "- **Liquid Time-constant Networks** - Hasani et al. (2020). AAAI. [arXiv:2006.04439](https://arxiv.org/abs/2006.04439)\n",
    "- **TinyStories: How Small Can Language Models Be and Still Speak Coherent English?** - Eldan & Li (2023). [arXiv:2305.07759](https://arxiv.org/abs/2305.07759)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
